---
layout: ue
title: 分布式文件存储SeaweedFS试用对比总结
category: 中间件技术
tags: SeaweedFS,文件存储
keywords: "SeaweedFS试用对比总结,分布式文件存储"
---

<p><span style="color: inherit; font-family: 黑体, SimHei; font-size: 24px; font-weight: 600;">基础概念</span><br/></p><p>1、SeaweedFS将磁盘进行了分组</p><p>&nbsp;&nbsp;&nbsp;&nbsp;分为DataCenters（数据中心、机房）、Racks（机架），Servers 和 Hard Drive，从而保证可用性。</p><p>2、<a href="https://github.com/chrislusf/seaweedfs/wiki/Replication" target="_blank">Replication</a>&nbsp;- 复制多副本</p><p>这是启动Master节点时设置的参数：</p><pre class="brush:bash;toolbar:false">./weed&nbsp;master&nbsp;-defaultReplication=001</pre><p>代表在相同机架的不同服务器上复制一个副本（共2两份）。</p><p>为什么是001，官方的定义如下：<br/></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p>000<span style="white-space: pre;"></span>no replication, just one copy</p></li><li><p>001<span style="white-space:pre"></span>replicate once on the same rack</p></li><li><p>010<span style="white-space:pre"></span>replicate once on a different rack in the same data center</p></li><li><p>100<span style="white-space:pre"></span>replicate once on a different data center</p></li><li><p>200<span style="white-space:pre"></span>replicate twice on two other different data center</p></li><li><p>110<span style="white-space: pre;"></span>replicate once on a different rack, and once on a different data center</p></li></ul><p>即，xyz三位数分别为：</p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p>x<span style="white-space: pre;"></span>number of replica in <strong>other data centers</strong></p></li><li><p>y<span style="white-space:pre"></span>number of replica in <strong>other racks</strong> in the same data center</p></li><li><p>z<span style="white-space: pre;"></span>number of replica in <strong>other servers</strong> in the same rack</p></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;通常，在测试环境，服务器都是一个机架上的，所以 xy 都为0。</p><p><br/></p><p><strong>组成部分：</strong></p><ol class=" list-paddingleft-2"><li><p>基础部分：Master server +&nbsp;Volume server</p></li><li><p>扩展部分：Filer server +&nbsp;Cronjob server (Replication-job) +&nbsp;S3 server</p></li></ol><p>&nbsp;&nbsp;&nbsp;&nbsp;各部分的作用稍后再说。<br/></p><p>&nbsp;&nbsp;&nbsp;&nbsp;值得注意的一点：外部与 Master Server、Volume Server 和 Filer 进行通信的方式是 HTTP API。API的用法官网有<a href="https://github.com/chrislusf/seaweedfs/wiki/Master-Server-API" target="_self">详细说明</a>。</p><p><br/></p><p><strong>概念对应关系</strong></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p>Node&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;系统抽象的节点，抽象为DataCenter、Rack、DataNode</p></li><li><p>DataCenter&nbsp; &nbsp;数据中心，对应现实中的不同机房</p></li><li><p>Rack&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;机架，对应现实中的机柜</p></li><li><p>Datanode&nbsp; &nbsp; &nbsp; 存储节点，用于管理、存储逻辑卷</p></li><li><p>Volume&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 逻辑卷，存储的逻辑结构，逻辑卷下存储Needle</p></li><li><p>Needle&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 逻辑卷中的Object，对应存储的文件（每个文件有一个唯一needleID）</p></li><li><p>Collection&nbsp; &nbsp; &nbsp; 文件集，可以分布在多个逻辑卷上</p></li></ul><p>注意，以上说的Datanode，其实就是Volume server（卷服务器），而Volume server下是有很多个逻辑卷的。</p><p><br/></p><p>&nbsp;&nbsp;&nbsp;&nbsp;官方文档关于 volume和 collection 的描述非常少，以下很多是我看了很多文档后摸索出来的。<br/></p><p><br/></p><p><strong>增大并发写和读 - Increase concurrent writes</strong></p><p>&nbsp; &nbsp; By default, SeaweedFS grows the volumes automatically. For example, for no-replication volumes, there will be concurrently 7 writable volumes allocated.</p><p>&nbsp;&nbsp;&nbsp;&nbsp;默认情况下，seaweedf会自动增加卷。例如，对于没有replication的卷，将同时分配7个可写卷。</p><p>&nbsp; &nbsp; If you want to distribute writes to more volumes, you can do so by instructing SeaweedFS master via this URL.</p><pre class="brush:bash;toolbar:false">curl&nbsp;http://localhost:9333/vol/grow?count=12&amp;replication=001</pre><p>&nbsp; &nbsp; This will assign 12 volumes with 001 replication. Since 001 replication means 2 copies for the same data, this will actually consumes 24 physical volumes（实际将消耗24个物理卷）。译者注：虽然volumes是逻辑结构，但是也是存放文件的地方，所以此处说物理卷，上面说逻辑卷，其实都是一个意思。</p><p><br/></p><p>&nbsp;&nbsp;&nbsp;&nbsp;另外，Seaweedf 在 卷（Volume） 上实现空间回收（删除掉的文件，卷会自动缩小），<a href="https://github.com/chrislusf/seaweedfs/wiki/Master-Server-API#force-garbage-collection" target="_blank">官方原文</a>如下：</p><p>&nbsp;&nbsp;&nbsp;&nbsp;If your system has many deletions, the deleted file&#39;s disk space will not be synchronously re-claimed. There is a background job to check volume disk usage. If empty space is more than the threshold, default to 0.3, the vacuum job will make the volume readonly, create a new volume with only existing files, and switch on the new volume. If you are impatient or doing some testing, vacuum the unused spaces this way.</p><p>&nbsp;&nbsp;&nbsp;&nbsp;也就是说，删除文件不会立即释放磁盘空间，只有卷的占用率低于阈值（0.3）才会触发释放空间的操作，而释放空间的手段是新建一个只包含未删除文件的新卷。<br/></p><p><strong>Pre-Allocate Volumes（预创建卷）</strong></p><p>&nbsp;&nbsp;&nbsp;&nbsp;One volume serves one write a time. If you need to increase concurrency, you can pre-allocate lots of volumes. Here are examples. You can combine all the different options also.</p><p>&nbsp;&nbsp;&nbsp;&nbsp;但是要注意，创建卷有很多属性，例如指定副本数、集合、数据中心等，参见<a href="https://github.com/chrislusf/seaweedfs/wiki/Master-Server-API#pre-allocate-volumes" target="_self">官方文档</a>。<br/></p><p><br/></p><h2>各组成部分介绍</h2><p><br/></p><h3><strong>S3 Server（适配Amazon S3 API）</strong></h3><p>&nbsp;&nbsp;&nbsp;&nbsp;To be compatible with Amazon S3 API, a separate &quot;weed s3&quot; command is provided. weed s3 will start a stateless gateway server to bridge the Amazon S3 API to SeaweedFS Filer.&nbsp;</p><p>&nbsp;&nbsp;&nbsp;&nbsp;For convenience, weed server -s3 will start a master, a volume server, a filer, and the S3 gateway.</p><p>&nbsp;&nbsp;&nbsp;&nbsp;Each bucket is stored in one collection, and mapped to folder /buckets/&lt;bucket_name&gt; by default.<br/></p><p>&nbsp;&nbsp;&nbsp;&nbsp;每个bucket会映射到&nbsp;/buckets/&lt;bucket_name&gt; 文件夹（应该是一个collection吧？collection就是folder？）。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;A bucket can be deleted efficiently by deleting the whole collection.<br/></p><p>&nbsp;&nbsp;&nbsp;&nbsp;Currently, the following APIs are supported.</p><pre class="brush:plain;toolbar:false">//&nbsp;Object&nbsp;operations
*&nbsp;PutObject
*&nbsp;GetObject
*&nbsp;HeadObject
*&nbsp;DeleteObject
*&nbsp;ListObjectsV2
*&nbsp;ListObjectsV1

//&nbsp;Bucket&nbsp;operations
*&nbsp;PutBucket
*&nbsp;DeleteBucket
*&nbsp;HeadBucket
*&nbsp;ListBuckets

//&nbsp;Multipart&nbsp;upload&nbsp;operations
*&nbsp;NewMultipartUpload
*&nbsp;CompleteMultipartUpload
*&nbsp;AbortMultipartUpload
*&nbsp;ListMultipartUploads</pre><p><br/></p><h3><strong>Filer Server - 文件管理器</strong></h3><p>&nbsp;&nbsp;&nbsp;&nbsp;文件管理器（Filer）可以用来 浏览文件和目录，以及add/delete files, and even browse the sub directories and files，还有检索、重命名等。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;Filer has a persistent client connecting to Master, to get the location updates of all volumes. 因此一个master server节点，只能部署一个Filer，</p><p>&nbsp;&nbsp;&nbsp;&nbsp;官方说明参见：<a href="https://github.com/chrislusf/seaweedfs/wiki/Directories-and-Files#architecture">https://github.com/chrislusf/seaweedfs/wiki/Directories-and-Files#architecture</a></p><p>&nbsp;&nbsp;&nbsp;&nbsp;weed mount 功能需要配合 Filer 才能使用，这样可以在服务器上用命令行操作文件。支持的操作如下：<br/></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p>file read / write</p></li><li><p>create new file</p></li><li><p>mkdir</p></li><li><p>list</p></li><li><p>remove</p></li><li><p>rename</p></li><li><p>chmod</p></li><li><p>chown</p></li><li><p>soft link</p></li><li><p>display free disk space</p></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;另外，<a href="https://github.com/chrislusf/seaweedfs/wiki/Filer-Server-API" target="_blank">Filer的HTTP API</a>也可以用来，其功能如下：<br/></p><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>上传文件</strong>：<br/></p><pre class="brush:bash;toolbar:false">#&nbsp;Basic&nbsp;Usage:
&gt;&nbsp;curl&nbsp;-F&nbsp;file=@report.js&nbsp;&quot;http://localhost:8888/javascript/&quot;
{&quot;name&quot;:&quot;report.js&quot;,&quot;size&quot;:866,&quot;fid&quot;:&quot;7,0254f1f3fd&quot;,&quot;url&quot;:&quot;http://localhost:8081/7,0254f1f3fd&quot;}
&gt;&nbsp;curl&nbsp;&nbsp;&quot;http://localhost:8888/javascript/report.js&quot;&nbsp;&nbsp;&nbsp;#&nbsp;get&nbsp;the&nbsp;file&nbsp;content
...
#&nbsp;upload&nbsp;the&nbsp;file&nbsp;with&nbsp;a&nbsp;different&nbsp;name
&gt;&nbsp;curl&nbsp;-F&nbsp;file=@report.js&nbsp;&quot;http://localhost:8888/javascript/new_name.js&quot;
{&quot;name&quot;:&quot;report.js&quot;,&quot;size&quot;:866,&quot;fid&quot;:&quot;3,034389657e&quot;,&quot;url&quot;:&quot;http://localhost:8081/3,034389657e&quot;}</pre><p>&nbsp;&nbsp;&nbsp;&nbsp;注意，上传文件时，如果带了目录路径，则会自动递归创建目录。<br/></p><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>查看某个目录的文件（list）</strong>：<br/></p><pre class="brush:bash;toolbar:false">#&nbsp;list&nbsp;all&nbsp;files&nbsp;under&nbsp;/javascript/
curl&nbsp;&nbsp;-H&nbsp;&quot;Accept:&nbsp;application/json&quot;&nbsp;&quot;http://localhost:8888/javascript/?pretty=y&quot;
{
&nbsp;&nbsp;&quot;Directory&quot;:&nbsp;&quot;/javascript/&quot;,
&nbsp;&nbsp;&quot;Files&quot;:&nbsp;[
&nbsp;&nbsp;&nbsp;&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;name&quot;:&nbsp;&quot;new_name.js&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;fid&quot;:&nbsp;&quot;3,034389657e&quot;
&nbsp;&nbsp;&nbsp;&nbsp;},
&nbsp;&nbsp;&nbsp;&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;name&quot;:&nbsp;&quot;report.js&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;fid&quot;:&nbsp;&quot;7,0254f1f3fd&quot;
&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;],
&nbsp;&nbsp;&quot;Subdirectories&quot;:&nbsp;null
}</pre><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>删除文件和目录</strong>：<br/></p><pre class="brush:bash;toolbar:false">curl&nbsp;-X&nbsp;DELETE&nbsp;http://localhost:8888/path/to/file
curl&nbsp;-X&nbsp;DELETE</pre><p><a href="http://localhost:8888/path/to/dir?recursive=true">http://localhost:8888/path/to/dir?recursive=true</a></p><p><br/></p><h3><strong>Cronjob server (Replication-job)&nbsp;</strong></h3><p>&nbsp;&nbsp;&nbsp;&nbsp;简单的讲，在运行大型群集时，通常会添加更多卷服务器，或者某些卷服务器关闭，或者某些卷服务器被替换。这些拓扑更改可能导致卷副本丢失或卷服务器上的卷数不平衡。所以这时就需要Cronjob server。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;When running large clusters, it is common that some volume servers are down. If a volume is replicated and one replica is missing, the volume will be marked as readonly.</p><p>&nbsp;&nbsp;&nbsp;&nbsp;One way to fix is to find one healthy copy and replicated to other servers, to meet the replication requirement. This volume id will be marked as writable.</p><p>&nbsp;&nbsp;&nbsp;&nbsp;In weed shell, the command volume.fix.replication will do exactly that, automating the replication fixing process. <strong>You can start a crontab job to periodically run volume.fix.replication to ensure the system health</strong>.</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在Cronjob server上运行着Replication-job，它会自动执行replication fixing操作。</p><p>&nbsp;&nbsp;&nbsp;&nbsp; 官方文档：<a href="https://github.com/chrislusf/seaweedfs/wiki/Volume-Management">https://github.com/chrislusf/seaweedfs/wiki/Volume-Management</a></p><p><br/></p><h3><strong>VolumeServer 卷服务器</strong></h3><p>&nbsp;&nbsp;&nbsp;&nbsp;这个就是所谓的“Data Node”数据节点，用于挂载磁盘存储文件。Volume Server与Master Server通信，受Master控制。可以动态的增加和减少VolumeServer，这一点比另一个云存储MinIO要强得多。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/chrislusf/seaweedfs/wiki/Volume-Server-API" target="_blank">卷服务器的API</a>主要功能为：<br/></p><p><strong>&nbsp;&nbsp;&nbsp;&nbsp;上传文件</strong>：</p><pre class="brush:bash;toolbar:false">curl&nbsp;-F&nbsp;file=@/home/chris/myphoto.jpg&nbsp;http://127.0.0.1:8080/3,01637037d6
{&quot;size&quot;:&nbsp;43234}</pre><p>注意，上传文件前，需要从master server取得 <strong>预分配的fileId</strong>。</p><p><strong>&nbsp;&nbsp;&nbsp;&nbsp;删除文件</strong>：<br/></p><pre class="brush:bash;toolbar:false">curl&nbsp;-X&nbsp;DELETE&nbsp;http://127.0.0.1:8080/3,01637037d6</pre><p><strong>&nbsp;&nbsp;&nbsp;&nbsp;访问/下载文件</strong>：</p><pre class="brush:bash;toolbar:false">curl&nbsp;http://127.0.0.1:8080/3,01637037d6</pre><p><br/></p><p>&nbsp;&nbsp;&nbsp;&nbsp;需要注意的是：通过VolumeServer或者MasterServer直接上传到自定义collection的文件，通过Filer默认的collection是访问不了的。通过Filer上传的文件所在的collection为 &quot;&quot;（空）。也许filer切换到指定collection才能访问里面的文件，但是我没研究过filer，官方也没有说明，可能需要摸索一下。</p><p><br/></p><h3><strong>Master Server</strong></h3><p>&nbsp;&nbsp;&nbsp;&nbsp;Master是不存储数据的，只做集群协调，类似于Zookeeper的作用吧。<br/></p><p>&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/chrislusf/seaweedfs/wiki/Master-Server-API" target="_blank">Master Server API</a>功能如下：</p><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>分配一个fileId，用于接下来的存储文件</strong><br/></p><pre class="brush:bash;toolbar:false">#&nbsp;Basic&nbsp;Usage:
curl&nbsp;http://localhost:9333/dir/assign
{&quot;count&quot;:1,&quot;fid&quot;:&quot;3,01637037d6&quot;,&quot;url&quot;:&quot;127.0.0.1:8080&quot;,
&nbsp;&quot;publicUrl&quot;:&quot;localhost:8080&quot;}
#&nbsp;To&nbsp;assign&nbsp;with&nbsp;a&nbsp;specific&nbsp;replication&nbsp;type:
curl&nbsp;&quot;http://localhost:9333/dir/assign?replication=001&quot;
#&nbsp;To&nbsp;specify&nbsp;how&nbsp;many&nbsp;file&nbsp;ids&nbsp;to&nbsp;reserve
curl&nbsp;&quot;http://localhost:9333/dir/assign?count=5&quot;
#&nbsp;To&nbsp;assign&nbsp;a&nbsp;specific&nbsp;data&nbsp;center
curl&nbsp;&quot;&nbsp;
另外，还可以指定文件集（collection）</pre><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>分配fileId+上传文件</strong>一次搞定：</p><pre class="brush:bash;toolbar:false">curl&nbsp;-F&nbsp;file=@/home/chris/myphoto.jpg&nbsp;http://localhost:9333/submit
{&quot;fid&quot;:&quot;3,01fbe0dc6f1f38&quot;,&quot;fileName&quot;:&quot;myphoto.jpg&quot;,&quot;fileUrl&quot;:&quot;localhost:8080/3,01fbe0dc6f1f38&quot;,&quot;size&quot;:68231}</pre><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>删除collection（文件集）</strong><br/></p><pre class="brush:bash;toolbar:false">#&nbsp;delete&nbsp;a&nbsp;collection
curl&nbsp;&quot;http://localhost:9333/col/delete?collection=benchmark&amp;pretty=y&quot;</pre><p><br/></p><h2>各种语言的客户端</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;官方没有提供客户端，只提供了API，各种语言可以自行封装客户端。<br/></p><p><br/></p><h2>试用简单总结</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;我使用了Docker Compose部署。在虚拟机的宿主机外访问Master，得到的Master返回的master leader url和volume server url都是docker容器本地IP，从在虚拟机的宿主机外无法访问。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;这个问题，作者好像不太care，不打算解决。当然这是个比较常见的问题，我通过改造client，做docker 内部ip和外部IP的映射也能解决问题。<br/></p><p>&nbsp;&nbsp;&nbsp;&nbsp;另外，官方没提供集群的docker-compose配置，下面是我用的一个集群配置：</p><p>&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/shiguanghuxian/seaweedfs-docker">https://github.com/shiguanghuxian/seaweedfs-docker</a></p><p>&nbsp;&nbsp;&nbsp;&nbsp;文件上传下载，S3 API这些我都测试过了，OK，唯一要吐槽一下的是，官方文档和教程不够全面，但考虑到作者也挺辛苦的，没多少资助，也就算了，只能慢慢等社区完善吧，现在看社区还是比较活跃的。<br/></p><p><br/></p><p><strong>FastDFS和SeaweedFS都有的一个问题</strong>：</p><p>&nbsp; &nbsp; FastDFS由tracker server 和 nodeserver组成，客户端的配置文件中只需要配置tracker server ip，然后tracker server会告诉客户端去访问哪个nodeserver，相当于tracker server是个中介服务。</p><p>&nbsp; &nbsp; 同样，SeaweedFS也有这个问题，先访问master，master再返回volume server的ip。</p><p>&nbsp; &nbsp; 这样会存在一个<strong>很普遍的问题</strong>：<span style="color: rgb(192, 0, 0);">访问tracker/master通常是用的外部ip，而tacker和node server（或master和volume server）之间的通信是用的本地ip。tracker/master当中介，返回的是本地ip，外部自然访问不了。</span></p><p><br/></p><p>解决方案1（需要中间件支持）</p><p>&nbsp; &nbsp; 中介服务器能够拿到节点服务器的外网ip，然后返回。</p><p>解决方案2（客户端自己配置）</p><p>&nbsp; &nbsp; 客户端配置一个内网、外网ip地址和端口的映射关系，客户端获得内网URL后，替换成外网URL去访问。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;我找了一个SeaweedFS的Java客户端，按照这种思路改了一下，测试通过。<br/></p><p><br/></p>